{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENG 550 Final Project\n",
    "### Use the Amazon Appliances reviews dataset to develop a classifier or sentiment analyzer that can predict whether a given review is favorable or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Our project uses the Amazon Appliances [reviews dataset](https://amazon-reviews-2023.github.io/) to develop a sentiment analyzer classification model. By combining star ratings and textual content, the model is trained to predict whether a review leans positively or negatively towards a product. This approach offers a way to quickly grasp the general sentiment of a product and assist shoppers in filtering through large volumes of product feedback more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "### Selected Problem\n",
    "\n",
    "The problem aims to distinguish between favourable and unfavourable appliance reviews based on their text and accompanying reviews.\n",
    "\n",
    "### Why is it Important?\n",
    "\n",
    "Spending time reading review after review on a product becomes a burden. It is easy to misinterpret the mood behind a set of comments online which can easily lead to poor purchase decisions. A quick, automated sentiment indicator can ease the burden and establish a neutral decision making process.\n",
    "\n",
    "### What have Others Done in this Space?\n",
    "\n",
    "Researchers have performed [sentiment analysis](https://medium.com/@nafisaidris413/a-beginners-guide-for-product-review-sentiment-analysis-0de1f451167d) using Machine Learning and Natural Language Processing to automatically classify reviews as positive, negative, or neutral. Not only has sentiment analysis been applied to product reviews, it has also been applied to [social media](https://buffer.com/social-media-terms/sentiment-analysis) to determine how people perceive and talk about products and brands. This proves that data-driven classifiers are able to provide sentiment analysis scores for assist people in their daily lives, whether its to determine how their personal brand is viewed or how a to make an informed purchase through product review.\n",
    "\n",
    "### Existing gaps?\n",
    "\n",
    "Current solutions rely on product ratings to provide consumers with a sense of trust and quality to help them make purchasing decisions. This can be seen simply by going to any Amazon product and checking the reviews. Some reviews are informative with many positives about the product, though the product receives less than a rating of 5-stars, or the review is not informative whatsoever with a rating of 5-stars. Other times the reviews are clearly biased or the customer who leaves the review is disgruntled, leading to a 1- or 2-star rating. Using a combination of product star rating and textual review content, we are attempting to reveal patterns in product reviews that a rating alone might miss.\n",
    "\n",
    "### Data Analysis Questions\n",
    "\n",
    "1. Does text-based features add value beyond just a numerical rating?\n",
    "2. Are there certain words which portray a stronger positve or negative sentiment?\n",
    "3. How will adding text preprocessing impact accuracy?\n",
    "4. Which models work best with this data?\n",
    "\n",
    "### What is Proposed\n",
    "\n",
    "We are proposing a text classification pipeline that merges a product's star rating and textual features.\n",
    "\n",
    "### What are your Main Findings?\n",
    "\n",
    "To determine customer opinion on various products within the Appliance category in Amazon's online store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "### Exploration of Data Features and Refinement of Feature Space\n",
    "\n",
    "In this section, we focused on understanding the raw data collected from the collected [datasets](https://amazon-reviews-2023.github.io/) and transform them into a format suitable for model training. We begin by loading the Amazon Appliance reviews dataset and its corresponding metadata. We will explore the structure of the data, examine the distribution of fields we are interested in (like ratings), and assess the overall quality of the text reviews associated with the products. After we gain a thorough understanding, we apply a series of preprocessing techniques to clean and refine the text data. The goal here is to ultimately develop a set of features that can be fed into a machine learning model for sentiment classification.\n",
    "\n",
    "#### Key Steps\n",
    "\n",
    "1. **Loading the Data:**\n",
    "We will loaf the `Appliances.jsonl` (reviews) and `meta_Appliances.jsonl` (metadata) using Apache Spark to avoid memory overload\n",
    "\n",
    "2. **Initial Inspection and Basic Statistics:**\n",
    "We will look at a few sample rows, check data types, count missing values, and examine distributions.\n",
    "\n",
    "3. **Textual Data Exploration:**\n",
    "We consider the nature of each review such as its length, the character composition, and common words. This should help guide our text cleaning decision.\n",
    "\n",
    "4. **Data Cleaning:**\n",
    "We clean the text by methods such as lowercasing the characters, removing punctuation, stripping leading and/or trailing whitespaces.\n",
    "\n",
    "5. **Feature Transformation:**\n",
    "We will use Spark Machine Learning's feature extraction tools to convert raw text into numeric features that are typically suitable for machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Data\n",
    "\n",
    "The datasets are provided in `*.jsonl` format, which means each line is a separate JSON object representing a single review or product's metadata. We will use `SparkSession` to read the files which will handle the data in a distributed manner, effectively avoiding a potential kernel crash. Spark's lazy evaluation, transformations, and actions will manage memory usage of the large `.jsonl` files.\n",
    "\n",
    "The two main data sources:\n",
    "1. **Review File (`Appliances.jsonl`):**\n",
    "Contains user-level reviews with fields such as `rating`, `title`, `text`, and `helpful_vote`.\n",
    "\n",
    "2. **Metada File (`meta_Appliaces.jsonl`):**\n",
    "Contains product-level information like `main_category`, `average_rating`, and `price`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import col, isnan, when, count, expr, sum, size, lower, regexp_replace, min, avg, max\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, IntegerType, ArrayType, MapType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a Spark Session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .master(\"local[*]\")     \n",
    "        .appName(\"Amazon Review Analysis\")\n",
    "        .config(\"spark.driver.memory\", \"4g\")\n",
    "        .config(\"spark.executor.memory\", \"4g\")\n",
    "        .config(\"spark.driver.host\", \"127.0.0.1\")  \n",
    "        .config(\"spark.driver.port\", \"4040\")       \n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "def create_schema(fields):\n",
    "    return StructType([StructField(name, dtype, True) for name, dtype in fields])\n",
    "\n",
    "# Only using columns needed for analysis for Reviews\n",
    "reviews_schema = create_schema([\n",
    "    (\"rating\", FloatType()),\n",
    "    (\"title\", StringType()),\n",
    "    (\"text\", StringType()),\n",
    "    (\"helpful_vote\", IntegerType()),\n",
    "    (\"asin\", StringType()),\n",
    "    (\"parent_asin\", StringType())\n",
    "])\n",
    "\n",
    "# Only using columns needed for analysis for Metadata\n",
    "meta_schema = create_schema([\n",
    "    (\"main_category\", StringType()),\n",
    "    (\"title\", StringType()),\n",
    "    (\"average_rating\", FloatType()),\n",
    "    (\"rating_number\", IntegerType()),\n",
    "    (\"price\", FloatType()),\n",
    "    (\"categories\", ArrayType(StringType())),\n",
    "    (\"parent_asin\", StringType())\n",
    "])\n",
    "\n",
    "# Point to the location where the .jsonl files are\n",
    "data_files = {\n",
    "    \"reviews\": \"./datasets/Appliances.jsonl\",\n",
    "    \"meta\": \"./datasets/meta_Appliances.jsonl\"\n",
    "    \n",
    "}\n",
    "\n",
    "# Use the schema when reading the JSON file for Reviews\n",
    "df_reviews = spark.read.schema(reviews_schema).json(data_files[\"reviews\"])\n",
    "\n",
    "# Use the schema when reading the JSON file for Meta\n",
    "df_meta = spark.read.schema(meta_schema).json(data_files[\"meta\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Inspection & Basic Statistics\n",
    "\n",
    "First it is important to understand the size of the dataset we are dealing with and the distribution of ratings. Using Spark actions like `show()` and `count()` we determine some initial statistics about both datasets which will be helpful to visualize them. We also would like to know how many values in each column of the datasets are `null`, `None`, and `NaN`, in case we need to do some backfilling or should ignore those sets completely. We can also display a few rows of each dataset which makes sure that the datasets were loaded successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Structure Inspection\n",
    "\n",
    "For the structure of each dataset we will check the datasets dimensions, schema, and preview the data to understand each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Structure Inpsection Function\n",
    "\n",
    "def structure_inspection(df, name):\n",
    "    # Print Dimensions\n",
    "    print(f\"{name} Dimensions: {df.count()} rows, {len(df.columns)} columns\")\n",
    "    \n",
    "    # Print Schema\n",
    "    print(f\"\\n{name} Schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    # Preview Data\n",
    "    print(f\"\\n{name} Preview:\")\n",
    "    df.show(10, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Reviews\n",
    "structure_inspection(df_reviews, \"Appliance Reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Metadata\n",
    "structure_inspection(df_meta, \"Appliance Metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values Inspection\n",
    "\n",
    "Missing values is one of the most common headaches in datasets. We need to check for them in both datasets to ensure that we can confidently use the data, else we have to consider backfilling the missing data or not using it at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nulls_counter(df, col_dtypes):\n",
    "    null_dfs = []\n",
    "    for col_dtype, cols in col_dtypes.items():\n",
    "        if col_dtype in [\"float\", \"integer\"]:\n",
    "            null_dfs.append(df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in cols]))\n",
    "        elif col_dtype == \"string\":\n",
    "            null_dfs.append(df.select([sum((col(c).isNull() | (col(c) == \"\")).cast(\"int\")).alias(c) for c in cols]))\n",
    "        elif col_dtype == \"array\":\n",
    "            null_dfs.append(df.select([sum((col(c).isNull() | expr(f\"exists({c}, x -> x == '')\")).cast(\"int\")).alias(c) for c in cols]))\n",
    "\n",
    "    # Combine all null DataFrames using reduce\n",
    "    return reduce(lambda df1, df2: df1.crossJoin(df2), null_dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_missing_values(df, name):\n",
    "    col_dtypes = {\n",
    "        \"float\": [c for c in df.columns if df.schema[c].dataType.simpleString() == \"float\"],\n",
    "        \"string\": [c for c in df.columns if df.schema[c].dataType.simpleString() == \"string\"],\n",
    "        \"integer\": [c for c in df.columns if df.schema[c].dataType.simpleString() == \"int\"],\n",
    "        \"array\": [c for c in df.columns if df.schema[c].dataType.simpleString().startswith(\"array\")]\n",
    "    }\n",
    "    \n",
    "    null_counter = get_nulls_counter(df, col_dtypes)\n",
    "    \n",
    "    print(f\"{name} Counted Missing Values per Column:\")\n",
    "    null_counter.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_missing_values(df_reviews, \"Appliance Reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_missing_values(df_meta, \"Appliance Metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicates\n",
    "\n",
    "Catching if there is duplicate data is important so that we do not have skewed data. All the data should be unique. Duplicate data will be handled during data cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_data(df, name):\n",
    "    total_count = df.count()\n",
    "    distinct_count = df.distinct().count()\n",
    "    duplicate_count = total_count - distinct_count\n",
    "    print(f\"{name} Duplicate Data: {duplicate_count}\\n(Total: {total_count}, Distinct: {distinct_count})\")       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_data(df_reviews, \"Appliance Reviews\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_data(df_meta, \"Appliance Metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Data\n",
    "\n",
    "We examine the statistical summaries for both the Reviews and Metadata. This helps us spot outliers and check if there are unexpected ranges that we have to look out for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Appliance Reviews Statistical Summary\")\n",
    "df_reviews.describe().show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Appliance Metadata Statistical Summary\")\n",
    "df_meta.describe().show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textual Data Exploration\n",
    "\n",
    "The main predictive feature of the model will likely be the `text` column in the Appliance Review dataset. It is important that we are able to understand its quality. We find answers to questions such as are the reviews too short or lone? Do they contain descriptive terms or just a few words? We also need to consider if there is text in different languages other than english.\n",
    "\n",
    "What we will do is start by analyzing the length of the reviews in terms of word count. This will help guide us in the direction we want. If the text is too short, maybe we need to rely more on ratings or metadata. If text is rich, a text-based sentiment analysis may just work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, size, length, avg, stddev, min, max, expr\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# Fill null text fields and cast text to string\n",
    "df_reviews = df_reviews.fillna({\"text\": \"\"}).withColumn(\"text\", col(\"text\").cast(\"string\"))\n",
    "\n",
    "# Tokenize text and calculate character length and word count\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words_raw\")\n",
    "df_tokenizer_reviews = (\n",
    "    tokenizer\n",
    "    .transform(df_reviews)\n",
    "    .withColumn(\"character_length\", length(col(\"text\")))\n",
    "    .withColumn(\"word_count\", size(col(\"words_raw\")))\n",
    "    .filter(col(\"character_length\") > 9)\n",
    ")\n",
    "\n",
    "# Show summary statistics for word count and character length\n",
    "print(\"Appliance Reviews with Word Count and Character Length Included\")\n",
    "df_tokenizer_reviews.show(10, truncate=True)\n",
    "\n",
    "print(\"\\nSummary Statistics for Appliance Reviews:\")\n",
    "df_tokenizer_reviews.select(\n",
    "    min(\"character_length\").alias(\"min_char_length\"),\n",
    "    avg(\"character_length\").alias(\"avg_char_length\"),\n",
    "    stddev(\"character_length\").alias(\"stddev_char_length\"),\n",
    "    max(\"character_length\").alias(\"max_char_length\"),\n",
    "    min(\"word_count\").alias(\"min_word_count\"),\n",
    "    avg(\"word_count\").alias(\"avg_word_count\"),\n",
    "    stddev(\"word_count\").alias(\"stddev_word_count\"),\n",
    "    max(\"word_count\").alias(\"max_word_count\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "It is important to remove the duplicate data and also empty data in the `text` column of the reviews data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_clean = df_reviews.dropDuplicates()\n",
    "\n",
    "df_reviews_clean = df_reviews_clean.filter(\n",
    "    (col(\"text\").isNotNull()) & (col(\"text\") != \"\")\n",
    ")\n",
    "\n",
    "print(\"Original Row Count:\", df_reviews.count())\n",
    "print(\"Cleaned Row Count:\", df_reviews_clean.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaned Reviews Dataset:\")\n",
    "df_reviews_clean.show(10, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "This step is similar to the ***Textual Data Exploration*** step, with the exception that it is performed on the cleaned data.\n",
    "\n",
    "This step is used to transform the `text` column of the reviews dataset into numerical features that a machine learning model can understand. We used ***TF-IDF (Term Frequency-Inverse Document Frequency)*** to represent the importance of words in each review while reducing the influence of commonly used words. This step involves tokenizing the text, removing stopwords, and applying TF-IDF transformation to create a feature vector for each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML tags from the 'text' column using regexp_replace\n",
    "df_reviews_clean = df_reviews_clean.withColumn(\n",
    "    \"text\", regexp_replace(col(\"text\"), \"<[^>]+>\", \"\")  # Remove anything between < and >\n",
    ")\n",
    "\n",
    "# Tokenization - Split the 'text' column into individual words\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words_raw\")\n",
    "df_reviews_tokenized = tokenizer.transform(df_reviews_clean)\n",
    "\n",
    "df_reviews_tokenized.show(10, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords Removal - Remove common words like \"the\", \"and\", etc.\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words_raw\", outputCol=\"words_cleaned\")\n",
    "df_reviews_cleaned_words = stopwords_remover.transform(df_reviews_tokenized)\n",
    "\n",
    "df_reviews_cleaned_words.show(10, truncate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer - Convert words into a term frequency vector\n",
    "count_vectorizer = CountVectorizer(inputCol=\"words_cleaned\", outputCol=\"raw_features\")\n",
    "cv_model = count_vectorizer.fit(df_reviews_cleaned_words)\n",
    "df_reviews_vectorized = cv_model.transform(df_reviews_cleaned_words)\n",
    "\n",
    "df_reviews_vectorized.show(10, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF - Apply Inverse Document Frequency to weigh word importance\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\")\n",
    "idf_model = idf.fit(df_reviews_vectorized)\n",
    "df_reviews_final = idf_model.transform(df_reviews_vectorized)\n",
    "\n",
    "df_reviews_final.show(10, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Verification\n",
    "print(\"Reviews Dataset with TF-IDF Features:\")\n",
    "df_reviews_final.select(\"text\", \"tfidf_features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Setup\n",
    "\n",
    "For the sentiment analysis task, we focused on predicting whether a given review is favorable (positive sentiment) or unfavorable (negative sentiment). We began by creating a binary label based on the rating column, where ratings ≥ 4 are considered favorable, and ratings < 4 are unfavorable. For quick experimentation, we’ll use Logistic Regression, a simple and effective model for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target label\n",
    "# Reviews with rating >= 4 are favorable (label=1), otherwise unfavorable (label=0)\n",
    "df_reviews_labeled = df_reviews_final.withColumn(\n",
    "    \"label\", when(col(\"rating\") >= 4, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "df_reviews_labeled.show(10, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (80-20 split)\n",
    "train_data, test_data = df_reviews_labeled.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"tfidf_features\", labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a pipeline (no additional transformations needed here)\n",
    "pipeline = Pipeline(stages=[lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "lr_model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
