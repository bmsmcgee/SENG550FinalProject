{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENG 550 Final Project\n",
    "### Use the Amazon Appliances reviews dataset to develop a classifier or sentiment analyzer that can predict whether a given review is favorable or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Our project uses the Amazon Appliances [reviews dataset](https://amazon-reviews-2023.github.io/) to develop a sentiment analyzer classification model. By combining star ratings and textual content, the model is trained to predict whether a review leans positively or negatively towards a product. This approach offers a way to quickly grasp the general sentiment of a product and assist shoppers in filtering through large volumes of product feedback more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "### Selected Problem\n",
    "\n",
    "The problem aims to distinguish between favourable and unfavourable appliance reviews based on their text and accompanying reviews.\n",
    "\n",
    "### Why is it Important?\n",
    "\n",
    "Spending time reading review after review on a product becomes a burden. It is easy to misinterpret the mood behind a set of comments online which can easily lead to poor purchase decisions. A quick, automated sentiment indicator can ease the burden and establish a neutral decision making process.\n",
    "\n",
    "### What have Others Done in this Space?\n",
    "\n",
    "Researchers have performed [sentiment analysis](https://medium.com/@nafisaidris413/a-beginners-guide-for-product-review-sentiment-analysis-0de1f451167d) using Machine Learning and Natural Language Processing to automatically classify reviews as positive, negative, or neutral. Not only has sentiment analysis been applied to product reviews, it has also been applied to [social media](https://buffer.com/social-media-terms/sentiment-analysis) to determine how people perceive and talk about products and brands. This proves that data-driven classifiers are able to provide sentiment analysis scores for assist people in their daily lives, whether its to determine how their personal brand is viewed or how a to make an informed purchase through product review.\n",
    "\n",
    "### Existing gaps?\n",
    "\n",
    "Current solutions rely on product ratings to provide consumers with a sense of trust and quality to help them make purchasing decisions. This can be seen simply by going to any Amazon product and checking the reviews. Some reviews are informative with many positives about the product, though the product receives less than a rating of 5-stars, or the review is not informative whatsoever with a rating of 5-stars. Other times the reviews are clearly biased or the customer who leaves the review is disgruntled, leading to a 1- or 2-star rating. Using a combination of product star rating and textual review content, we are attempting to reveal patterns in product reviews that a rating alone might miss.\n",
    "\n",
    "### Data Analysis Questions\n",
    "\n",
    "1. Does text-based features add value beyond just a numerical rating?\n",
    "2. Are there certain words which portray a stronger positve or negative sentiment?\n",
    "3. How will adding text preprocessing impact accuracy?\n",
    "4. Which models work best with this data?\n",
    "\n",
    "### What is Proposed\n",
    "\n",
    "We are proposing a text classification pipeline that merges a product's star rating and textual features.\n",
    "\n",
    "### What are your Main Findings?\n",
    "\n",
    "To determine customer opinion on various products within the Appliance category in Amazon's online store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "### Exploration of Data Features and Refinement of Feature Space\n",
    "\n",
    "In this section, we focused on understanding the raw data collected from the collected [datasets](https://amazon-reviews-2023.github.io/) and transform them into a format suitable for model training. We begin by loading the Amazon Appliance reviews dataset and its corresponding metadata. We will explore the structure of the data, examine the distribution of fields we are interested in (like ratings), and assess the overall quality of the text reviews associated with the products. After we gain a thorough understanding, we apply a series of preprocessing techniques to clean and refine the text data. The goal here is to ultimately develop a set of features that can be fed into a machine learning model for sentiment classification.\n",
    "\n",
    "#### Key Steps\n",
    "\n",
    "1. **Loading the Data:**\n",
    "We will loaf the `Appliances.jsonl` (reviews) and `meta_Appliances.jsonl` (metadata) using Apache Spark to avoid memory overload\n",
    "\n",
    "2. **Initial Inspection and Basic Statistics:**\n",
    "We will look at a few sample rows, check data types, count missing values, and examine distributions.\n",
    "\n",
    "3. **Textual Data Exploration:**\n",
    "We consider the nature of each review such as its length, the character composition, and common words. This should help guide our text cleaning decision.\n",
    "\n",
    "4. **Data Cleaning:**\n",
    "We clean the text by methods such as lowercasing the characters, removing punctuation, stripping leading and/or trailing whitespaces.\n",
    "\n",
    "5. **Feature Transformation:**\n",
    "We will use Spark Machine Learning's feature extraction tools to convert raw text into numeric features that are typically suitable for machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Data\n",
    "\n",
    "The datasets are provided in `*.jsonl` format, which means each line is a separate JSON object representing a single review or product's metadata. We will use `SparkSession` to read the files which will handle the data in a distributed manner, effectively avoiding a potential kernel crash. Spark's lazy evaluation, transformations, and actions will manage memory usage of the large `.jsonl` files\n",
    "\n",
    "The two main data sources:\n",
    "1. **Review File (`Appliances.jsonl`):**\n",
    "Contains user-level reviews with fields such as `rating`, `title`, `text`, `verified_purchase`, `helpful_vote`, and `timestamp`.\n",
    "\n",
    "2. **Metada File (`meta_Appliaces.jsonl`):**\n",
    "Contains product-level information like `main_category`, `average_rating`, `description`, `features`, and `price`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rating: float (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- asin: string (nullable = true)\n",
      " |-- parent_asin: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- verified_purchase: boolean (nullable = true)\n",
      " |-- helpful_vite: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- main_category: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- average_rating: float (nullable = true)\n",
      " |-- rating_number: integer (nullable = true)\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- description: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- store: string (nullable = true)\n",
      " |-- categories: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- details: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- parent_asin: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, types\n",
    "from pyspark.sql import functions\n",
    "from pyspark.ml import feature\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, IntegerType, ArrayType, MapType\n",
    "\n",
    "# Start a Spark Session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .master(\"local\")\n",
    "        .appName(\"Amazon Review Analysis\") \n",
    "        .config(\"spark.executor.memory\", \"4g\")\n",
    "        .config(\"spark.driver.memory\", \"4g\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "# Only using columns needed for analysis for Reviews\n",
    "reviews_schema = StructType([\n",
    "    StructField(\"rating\", FloatType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"parent_asin\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"verified_purchase\", BooleanType() , True),\n",
    "    StructField(\"helpful_vite\", IntegerType() , True)\n",
    "])\n",
    "\n",
    "# Only using columns needed for analysis for Metadata\n",
    "meta_schema = StructType([\n",
    "    StructField(\"main_category\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"average_rating\", FloatType(), True),\n",
    "    StructField(\"rating_number\", IntegerType(), True),\n",
    "    StructField(\"features\", ArrayType(StringType(), True), True),\n",
    "    StructField(\"description\", ArrayType(StringType(), True), True),\n",
    "    StructField(\"price\", FloatType(), True),\n",
    "    StructField(\"store\", StringType(), True),\n",
    "    StructField(\"categories\", ArrayType(StringType(), True), True),\n",
    "    StructField(\"details\", MapType(StringType(), StringType(), True), True),\n",
    "    StructField(\"parent_asin\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Point to the location where the .jsonl files are\n",
    "reviews_file = \"Appliances.jsonl\"\n",
    "meta_file = \"meta_Appliances.jsonl\"\n",
    "\n",
    "# Use the schema when reading the JSON file for Reviews\n",
    "df_reviews = spark.read.schema(reviews_schema).json(reviews_file)\n",
    "\n",
    "# Use the schema when reading the JSON file for Meta\n",
    "df_meta = spark.read.schema(meta_schema).json(meta_file)\n",
    "\n",
    "# Show the Schemas\n",
    "df_reviews.printSchema()\n",
    "df_meta.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schemas printed above gives an overview of the fields that are being used in the model for both the Appliances Reviews and Appliances Metadata. Spark will naturally run on **PORT 4040**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Inspection & Basic Statistics\n",
    "\n",
    "First it is important to understand the size of the dataset we are dealing with and the distribution of ratings. Using Spark actions like `show()` and `count()` we determine some initial statistics about both datasets which will be helpful to visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the Reviews dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Reviews:  2128605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct products:  104237\n",
      "From the Reviews dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:===================>                                      (1 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of Metadata 94327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"From the Reviews dataset\")\n",
    "print(f\"Number of Reviews: \", df_reviews.count())\n",
    "print(f\"Number of distinct products: \", df_reviews.select(\"asin\").distinct().count())\n",
    "\n",
    "print(f\"From the Meta dataset\")\n",
    "print(f\"Amount of Metadata\", df_meta.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
